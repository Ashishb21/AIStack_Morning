{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c782e6e-b69d-43cb-8a9f-9a4de21365d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (4.57.1)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (2.9.1)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from sentence-transformers) (1.8.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from requests->transformers) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Downloading sentence_transformers-5.2.0-py3-none-any.whl (493 kB)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Installing collected packages: accelerate, sentence-transformers\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [sentence-transformers]/2\u001b[0m [sentence-transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.12.0 sentence-transformers-5.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch sentence-transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7332bd15-3db4-48f2-a3d1-7724994282bc",
   "metadata": {},
   "source": [
    "## Encoder-Only Transformer (Text Classification / NER)\n",
    "### Example: Text Classification (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18091c85-98da-4f0b-8f92-f9b81cd85389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "text = \"This judgment was not followed by the High Court.\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "predicted_class = torch.argmax(logits, dim=1)\n",
    "\n",
    "print(\"Predicted class:\", predicted_class.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9cf0b3-1bd4-4b45-a2c2-5916b8a9b034",
   "metadata": {},
   "source": [
    "## Decoder-Only Transformer (Text Generation / LLM)\n",
    "### Example: Text Generation (GPT-2 / LLaMA-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffec5a8f-6763-4c1c-8e20-392e2b0e8be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain transformer models in simple words.\n",
      "\n",
      "The following is a list of the most common transformer models in the language.\n",
      "\n",
      "The following is a list of the most common transformer models in the language. The following is a list of the most common transformer models in the language. The following is a list of the most common transformer models in the language. The following is a list of the most common transformer models in the language. The following is a list of the most common transformer models in the\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Explain transformer models in simple words\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038b77b1-dc52-4be5-9e86-50948ba894a1",
   "metadata": {},
   "source": [
    "## Encoder–Decoder Transformer (Summarization / Translation)\n",
    "### Example: Summarization (T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38de642e-ff7a-40ed-874a-9596ece71d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: the prior ruling was not followed due to differing factual circumstances.... the supreme court observed that the prior ruling was not followed due to differing factual circumstances...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "text = \"\"\"\n",
    "The Supreme Court observed that the prior ruling was not followed \n",
    "due to differing factual circumstances...\n",
    "\"\"\"\n",
    "\n",
    "input_text = \"summarize: \" + text\n",
    "\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "summary_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_length=60,\n",
    "    min_length=20\n",
    ")\n",
    "\n",
    "print(\"Summary:\", tokenizer.decode(summary_ids[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0bf454-3c08-489d-a54f-d6eef76c3695",
   "metadata": {},
   "source": [
    "## Sentence Transformers (Embeddings / Semantic Search)\n",
    "### Example: Generate Embeddings (Best for RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abc75bc6-3a96-40d5-b9d0-cdeec6f031ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (2, 384)\n",
      "[[-3.92743051e-02  7.77428225e-02  3.10510863e-03 -3.67719941e-02\n",
      "   6.08768454e-03  2.79506464e-02  2.81491615e-02  1.01967551e-01\n",
      "   4.00985498e-03  4.17825729e-02  9.42809880e-02  1.25331953e-01\n",
      "   2.58782748e-02 -6.40802756e-02  3.72433811e-02  3.81202064e-02\n",
      "   9.40848738e-02  3.11784819e-02 -1.74372438e-02  2.60961466e-02\n",
      "  -2.99948407e-03  2.77408566e-02  2.01642560e-03  3.53095192e-03\n",
      "  -1.96987279e-02 -4.48261872e-02 -3.28719765e-02 -5.29057644e-02\n",
      "   2.23430656e-02  4.84758839e-02 -3.27321514e-02  1.03338681e-01\n",
      "  -4.17378219e-03  1.51731388e-03 -1.80717297e-02  8.04686453e-03\n",
      "   2.69205198e-02 -9.96926706e-03  5.83176166e-02  8.69004708e-03\n",
      "   6.22390658e-02 -1.11081405e-02  4.77364846e-02  3.57239209e-02\n",
      "  -8.49945024e-02  3.48009504e-02 -1.09764915e-02  3.07311527e-02\n",
      "  -3.39951292e-02  3.61061953e-02 -6.07582740e-02 -2.11984403e-02\n",
      "   5.02368324e-02  3.38226967e-02 -1.19538195e-02 -4.25465442e-02\n",
      "   2.96754427e-02  2.07581799e-02  4.30237092e-02  8.53249058e-03\n",
      "  -4.78518903e-02  1.70194842e-02 -3.03168651e-02  2.49044374e-02\n",
      "   6.42113686e-02 -3.01433392e-02  2.26252005e-02 -1.07616194e-01\n",
      "  -1.63388830e-02  4.24388535e-02  1.51400954e-01  3.68485302e-02\n",
      "  -1.86632723e-02 -4.09855023e-02 -4.78184372e-02 -1.95657667e-02\n",
      "   2.61833537e-02  4.50548828e-02  5.84117733e-02 -6.60818964e-02\n",
      "   2.37832218e-02 -6.29387423e-02 -3.13322172e-02 -1.14066765e-01\n",
      "   3.67221721e-02 -3.89371626e-02 -5.67473695e-02 -8.54124688e-03\n",
      "  -1.67497117e-02  7.35828769e-04  7.37621337e-02 -1.16678581e-01\n",
      "   4.45842966e-02 -3.27572897e-02 -1.71362031e-02 -5.39182834e-02\n",
      "  -5.46009466e-02  2.00324319e-02  1.52023872e-02  2.44693551e-02\n",
      "   1.90998998e-03  3.70956510e-02  1.05066514e-02  4.01471257e-02\n",
      "  -1.64872024e-03 -7.17383251e-02 -1.49602266e-02 -2.70044133e-02\n",
      "  -4.14054878e-02 -1.04786362e-02 -1.58695616e-02  1.73919704e-02\n",
      "   1.25626877e-01  2.94087399e-02 -1.53041892e-02  6.38627484e-02\n",
      "   3.65098044e-02  2.01604441e-02 -4.42065857e-02 -7.25850910e-02\n",
      "   1.93744563e-02  2.84943916e-02 -1.03992194e-01  4.52673109e-03\n",
      "  -6.81208149e-02  3.23491287e-03 -5.38553037e-02 -5.66051554e-33\n",
      "   7.16217756e-02 -5.31298257e-02  8.20242520e-03 -6.29421473e-02\n",
      "   6.32828996e-02 -7.78167471e-02 -4.58029844e-02 -2.78689098e-02\n",
      "   1.65216699e-02  6.19560387e-03 -9.32462979e-03 -4.72169463e-03\n",
      "   6.01543002e-02 -4.81969118e-02 -1.13005616e-01  3.32082026e-02\n",
      "  -7.99329877e-02  9.42861661e-02 -3.67405489e-02 -2.90416107e-02\n",
      "   4.08862382e-02 -4.29723896e-02  1.95213687e-02  5.43113388e-02\n",
      "  -1.28007025e-01 -9.90612246e-03  3.22393104e-02 -2.94149797e-02\n",
      "  -1.01310657e-02  8.88816174e-03  6.51523145e-03  2.34582950e-03\n",
      "   7.15713426e-02  7.03699365e-02  5.27829491e-02  4.91525605e-02\n",
      "   4.43847701e-02  3.30301858e-02  2.38968004e-02  6.96682408e-02\n",
      "  -9.33971908e-03  2.88611129e-02  9.07128602e-02  1.66749395e-02\n",
      "  -7.26030814e-03 -3.36700790e-02 -1.52426153e-01 -2.30853129e-02\n",
      "  -6.74904659e-02  4.06050086e-02  1.93956904e-02 -4.46665995e-02\n",
      "  -9.35367402e-03 -1.27079533e-02 -5.31366691e-02  5.71346767e-02\n",
      "  -7.90843591e-02  5.83207458e-02  4.96650785e-02  6.20060675e-02\n",
      "   3.76620777e-02  1.24774389e-01 -5.51170819e-02  9.39264968e-02\n",
      "  -6.14580959e-02  1.18556898e-02 -7.24829175e-03 -5.70557676e-02\n",
      "   5.31584164e-03  1.29022384e-02  9.71619561e-02  5.40737323e-02\n",
      "   4.62491661e-02  5.13932630e-02 -4.90773730e-02 -3.96510139e-02\n",
      "  -1.98475532e-02 -2.80750217e-03  3.55132110e-02 -3.67747508e-02\n",
      "  -5.36306091e-02 -1.24554038e-02  5.49341701e-02  3.84248570e-02\n",
      "  -7.79512385e-03 -1.69442836e-02 -1.18778504e-01 -6.95872232e-02\n",
      "  -7.73723423e-02  3.67779774e-03 -2.24150326e-02  2.30852868e-02\n",
      "  -2.43124031e-02 -2.03423891e-02  7.82376714e-03  3.52390578e-33\n",
      "  -7.67087564e-02 -6.32547289e-02  1.44327516e-02 -1.42116677e-02\n",
      "   1.11477254e-02 -4.17441130e-02 -2.15545874e-02 -4.95699458e-02\n",
      "  -3.88168320e-02  9.47720371e-03 -2.86420938e-02 -6.61669895e-02\n",
      "   1.62394252e-02  5.43836169e-02  2.70014983e-02 -2.76150294e-02\n",
      "   2.37588771e-03 -2.44815815e-02 -7.18400553e-02  1.67429205e-02\n",
      "   8.57132748e-02 -7.68437311e-02 -4.10794429e-02  5.13391979e-02\n",
      "  -4.24456894e-02 -8.28624517e-03  2.14570463e-02 -1.27474621e-01\n",
      "   1.90003850e-02  8.29427503e-03 -4.01308201e-02 -1.13918949e-02\n",
      "  -8.05599093e-02  4.50797155e-02 -3.66908945e-02 -1.40276067e-02\n",
      "   9.89715531e-02 -2.84634214e-02 -2.92244051e-02 -7.71175325e-02\n",
      "   8.76165088e-03  2.85335295e-02  1.29695712e-02  1.21532930e-02\n",
      "   5.26738949e-02  4.18083929e-02 -1.14372466e-02  1.79507863e-02\n",
      "   1.20847911e-01 -2.75578592e-02 -5.11244535e-02 -3.03073023e-02\n",
      "   6.96253031e-02  1.11473866e-01 -5.18412590e-02 -5.88497855e-02\n",
      "  -4.94056940e-03 -4.97130975e-02 -1.85376965e-02 -4.45924029e-02\n",
      "   7.55241960e-02  7.44083449e-02 -1.46034017e-01  4.24926821e-03\n",
      "   6.24373369e-02  3.16820405e-02 -8.02187100e-02 -4.97090705e-02\n",
      "   1.74695086e-02 -2.22779661e-02 -2.65807994e-02 -6.87744319e-02\n",
      "   1.46659771e-02 -4.05382589e-02  2.27560177e-02  4.73472141e-02\n",
      "  -8.28987546e-03  8.78406987e-02 -3.83029543e-02  5.18045621e-03\n",
      "   6.84521198e-02 -3.36281210e-02 -6.10762984e-02  1.03405274e-01\n",
      "   6.69568684e-03  4.34334204e-02  1.52856009e-02 -9.21366438e-02\n",
      "  -1.55115863e-02  4.51153284e-03 -2.99563669e-02 -4.16652337e-02\n",
      "  -4.51531410e-02  2.60933917e-02 -9.42870043e-03 -1.73382855e-08\n",
      "  -1.22176744e-02  6.65679649e-02 -3.65198515e-02 -1.04178600e-02\n",
      "   1.68842021e-02  2.80719008e-02  5.16864955e-02  5.14263101e-02\n",
      "   6.85937842e-03  1.32691870e-02 -9.40928757e-02  1.79725699e-02\n",
      "  -9.97173600e-04 -7.17205256e-02  5.56968199e-03 -3.53813991e-02\n",
      "   2.78052296e-02 -4.60822098e-02 -3.49115953e-03  3.74001972e-02\n",
      "  -4.32760306e-02  1.83617380e-02  3.54257487e-02  2.02306136e-02\n",
      "  -9.70542617e-03 -1.16941193e-03 -2.61607114e-03  1.55631928e-02\n",
      "  -2.81858258e-02  2.46139821e-02 -3.86320613e-02  7.07648247e-02\n",
      "  -5.03998324e-02 -5.77661283e-02  5.13676479e-02 -1.33862728e-02\n",
      "  -3.21928784e-03  5.80296218e-02  5.57220317e-02 -5.76453358e-02\n",
      "  -7.31501356e-02  4.29925136e-03  1.23111624e-03  4.99327667e-02\n",
      "   1.17982566e-01 -2.95854844e-02 -8.98281261e-02  3.86630483e-02\n",
      "   3.38860676e-02  3.30517516e-02 -6.90949708e-03 -5.52304834e-02\n",
      "   5.95479421e-02 -4.41067107e-02 -1.13307303e-02  4.72466610e-02\n",
      "   8.12898055e-02 -4.03721184e-02 -5.50499409e-02 -6.93495348e-02\n",
      "   1.63946241e-01  2.88548749e-02  7.00529218e-02  1.11375719e-01]\n",
      " [ 1.86885393e-03  1.10399880e-01 -2.49119774e-02  2.25548111e-02\n",
      "   4.50783186e-02 -1.66773126e-02  6.70422241e-02  2.18276605e-02\n",
      "   7.57155614e-03  7.85451904e-02  8.56321380e-02  8.63655731e-02\n",
      "  -1.50978621e-02 -1.38976639e-02 -3.97217236e-02  2.92236004e-02\n",
      "  -2.54256204e-02 -3.59887294e-02 -6.13809451e-02  2.30099913e-02\n",
      "   7.12999552e-02  6.66640103e-02 -4.27181162e-02  2.41939686e-02\n",
      "   2.03416087e-02  2.56961621e-02 -8.01980346e-02 -2.81546712e-02\n",
      "   2.93771867e-02 -5.64197302e-02 -3.67302895e-02  7.87784159e-02\n",
      "  -4.90047000e-02 -7.55836396e-03 -3.94477434e-02  6.71357103e-03\n",
      "   3.05428952e-02 -4.75183036e-03 -3.01720258e-02 -4.49080542e-02\n",
      "   4.37108949e-02  2.57219970e-02 -3.60837393e-02  3.51933092e-02\n",
      "  -4.90221716e-02  2.00971449e-03  5.78953186e-03 -4.94550206e-02\n",
      "  -2.10821629e-02  2.53381319e-02 -2.76760906e-02  7.28869392e-03\n",
      "   1.17992619e-02 -2.83520366e-03 -4.14942205e-02  2.05644742e-02\n",
      "  -3.72870266e-02  5.18351793e-02  9.17084701e-03 -5.02797924e-02\n",
      "  -5.84543012e-02  8.10768735e-03 -3.40442955e-02  1.11311106e-02\n",
      "   5.41026108e-02 -2.70643309e-02  3.23724635e-02 -8.28954875e-02\n",
      "  -7.97908232e-02  8.46816748e-02  1.42822623e-01  7.36327376e-03\n",
      "   3.44905816e-02 -3.20484638e-02 -2.72973608e-02  2.37089936e-02\n",
      "   6.17591944e-03 -2.11579190e-03  3.77834886e-02 -8.66284147e-02\n",
      "  -5.32673672e-02 -2.65935697e-02 -7.81808700e-03 -8.97144377e-02\n",
      "   5.61684482e-02 -1.06346205e-01  4.22659963e-02 -8.40902328e-02\n",
      "   1.03399552e-01  6.10901713e-02  6.85405731e-02 -6.32044673e-02\n",
      "   5.84440902e-02  2.03436241e-02  2.63116974e-02 -5.21343239e-02\n",
      "  -6.25550374e-02  3.06528118e-02  2.58845445e-02  5.26216105e-02\n",
      "  -4.12672646e-02  6.37451634e-02 -9.18853134e-02  1.15184328e-02\n",
      "   5.55610098e-02 -1.79244038e-02 -2.31027678e-02 -3.80745567e-02\n",
      "  -1.05226589e-02 -2.80270465e-02  4.48911544e-03 -1.72912306e-03\n",
      "   8.99724588e-02  1.70913700e-03  4.85117696e-02  1.05166540e-01\n",
      "  -2.46033892e-02  7.68215507e-02  2.00138651e-02 -1.16280474e-01\n",
      "   5.60102947e-02  4.57718782e-02 -4.07104827e-02  1.83133278e-02\n",
      "  -5.53809814e-02 -2.87359133e-02 -1.13024041e-02 -2.72758623e-33\n",
      "  -2.68289587e-03 -8.12073946e-02 -8.07059407e-02 -5.86783066e-02\n",
      "   5.23153245e-02  1.46945817e-02 -1.03216078e-02  4.84626107e-02\n",
      "  -9.16023329e-02 -3.76987346e-02  4.56786379e-02 -5.79247735e-02\n",
      "  -5.43839522e-02 -1.16932377e-01 -5.50052412e-02  4.15240675e-02\n",
      "  -3.98131534e-02  6.35299459e-02  5.07181287e-02  2.67355684e-02\n",
      "   4.51429524e-02 -2.31694076e-02 -6.71540573e-03 -5.33752702e-03\n",
      "  -9.62494090e-02  8.31610058e-03  6.13357965e-03 -9.40699689e-03\n",
      "  -7.32912272e-02  2.44791675e-02  2.33233999e-02  5.50146438e-02\n",
      "   9.04373378e-02  9.40813944e-02  3.03119514e-02  7.49822259e-02\n",
      "  -4.24481742e-03 -2.14274731e-02  5.83108282e-03 -3.68319545e-03\n",
      "  -1.24434298e-02  2.63456274e-02 -1.30962431e-02 -2.26543602e-02\n",
      "  -4.74615395e-02 -3.39909643e-02 -6.85410872e-02  6.59648888e-03\n",
      "  -7.87416399e-02  4.74227183e-02  7.78789911e-03 -1.27190175e-02\n",
      "   9.03501287e-02  1.41950762e-02 -9.58003774e-02  3.32111232e-02\n",
      "   1.07813105e-02  6.61520138e-02 -1.31106963e-02  2.56402679e-02\n",
      "   4.30460609e-02  5.11120707e-02 -4.31809835e-02 -3.46195586e-02\n",
      "  -9.71186012e-02  3.22815478e-02 -2.65520010e-02  4.53539239e-03\n",
      "   1.79305412e-02 -6.92287460e-02 -6.98769540e-02  3.65449227e-02\n",
      "   1.14926668e-02  5.10462001e-02 -7.29585588e-02 -4.39334176e-02\n",
      "   1.04454318e-02  5.12490012e-02  4.95932586e-02 -6.98415041e-02\n",
      "   1.79543458e-02  1.80247836e-02  3.52968685e-02  4.09799553e-02\n",
      "   9.97555070e-03 -5.36074638e-02 -3.10010072e-02 -3.61277834e-02\n",
      "  -2.17474252e-02  3.49416099e-02 -2.72166487e-02  2.66591646e-02\n",
      "   7.21155927e-02 -7.78622739e-03 -1.51296956e-02  1.29464843e-33\n",
      "  -6.98155761e-02 -7.92345032e-02 -3.68974507e-02  6.05469085e-02\n",
      "  -4.92354482e-02 -7.40099847e-02 -5.79542704e-02 -7.38011003e-02\n",
      "  -6.61869273e-02 -9.94350463e-02 -2.83229128e-02  8.05741269e-03\n",
      "   9.76489112e-02  4.50921059e-02  3.44801173e-02  3.02132741e-02\n",
      "   2.71739569e-02 -3.99886072e-02 -2.16892865e-02 -1.16203064e-02\n",
      "   7.35909790e-02  4.70905937e-02  4.20286320e-03  8.44970420e-02\n",
      "  -5.63267469e-02  8.43448415e-02  1.21978028e-02 -1.26973242e-01\n",
      "   4.53271456e-02  2.00492726e-03  1.91853307e-02 -5.44648953e-02\n",
      "  -5.18702380e-02  1.01387110e-02  1.32194340e-01 -1.13614276e-02\n",
      "   6.10236228e-02 -2.13701911e-02  1.07686582e-03 -3.72470506e-02\n",
      "  -2.58855633e-02 -1.57712474e-02 -3.50329690e-02  4.61466908e-02\n",
      "   1.10961564e-01 -6.88262284e-02  5.33731915e-02  4.82341871e-02\n",
      "   9.27773789e-02 -4.85168360e-02 -2.24693324e-02 -4.06357236e-02\n",
      "   9.72313136e-02  9.33472887e-02 -7.83109814e-02 -6.21060766e-02\n",
      "   7.70633593e-02 -2.35916506e-02  9.49155241e-02 -1.57980546e-02\n",
      "   1.48579767e-02  2.92395614e-03  1.24113904e-02 -2.64316350e-02\n",
      "   3.41720730e-02  7.43125603e-02 -4.36153710e-02 -3.17348093e-02\n",
      "   7.34139755e-02  4.99127107e-03  2.66125984e-02 -5.88668585e-02\n",
      "  -1.05493717e-01 -3.58043276e-02  8.44314322e-02  1.24504462e-01\n",
      "  -3.04201599e-02  5.49685471e-02 -5.13902344e-02  3.32333483e-02\n",
      "  -2.00166344e-03 -3.34466137e-02 -4.97356942e-03 -2.33126506e-02\n",
      "   4.08251211e-02 -2.59061251e-02  4.18613367e-02 -6.86498359e-02\n",
      "   7.36676296e-03  3.17154229e-02 -2.40391064e-02 -7.64788762e-02\n",
      "   5.21032996e-02 -5.51802758e-03 -8.89778435e-02 -1.82647391e-08\n",
      "  -1.87517889e-02 -4.70906384e-02 -1.51105151e-02  6.11646101e-02\n",
      "   8.55767429e-02 -3.16803306e-02  5.20426407e-02  2.63904557e-02\n",
      "  -2.74102762e-02  1.37552898e-02  1.46029638e-02  6.36222288e-02\n",
      "   4.57315892e-02 -1.13343140e-02 -2.17685439e-02 -7.05122948e-02\n",
      "  -4.75426838e-02 -2.74799652e-02 -2.72909384e-02  6.99609816e-02\n",
      "  -4.97955196e-02 -3.06444950e-02 -5.01116775e-02 -9.65193957e-02\n",
      "  -1.40371453e-02 -9.65779927e-03 -3.56966928e-02  5.09449802e-02\n",
      "  -9.00034560e-04  2.87432149e-02  4.89922091e-02  7.88291246e-02\n",
      "  -4.21843193e-02  1.89057998e-02 -9.42901243e-04 -3.25798914e-02\n",
      "   2.83692610e-02  9.37456563e-02  2.85066590e-02  3.17286700e-03\n",
      "  -2.40961500e-02  6.24237210e-02  1.13338688e-02  8.86299014e-02\n",
      "   9.94251296e-02 -2.06749700e-02 -6.57857433e-02  9.73714329e-03\n",
      "  -4.17948328e-02 -4.99275560e-03  5.97277731e-02  1.31503288e-02\n",
      "   4.72614020e-02 -1.45055177e-02  4.07117456e-02 -6.36853352e-02\n",
      "   4.69218083e-02  2.99794413e-02 -6.36776015e-02  1.40103335e-02\n",
      "  -3.08660828e-02 -1.03380345e-01  6.64970931e-03  2.24094670e-02]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "sentences = [\n",
    "    \"The case was cited by the High Court.\",\n",
    "    \"This judgment was overruled later.\"\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "print(\"Embedding shape:\", embeddings.shape)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809bc3af-ea3f-4113-9fdc-e5edfe5b01ac",
   "metadata": {},
   "source": [
    "### Code-Focused Transformer (Code Understanding)\n",
    "## Example: Code Embeddings (CodeBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71f6d9fa-3bf6-495d-844a-b0707fbbf118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code embedding shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "code = \"def add(a, b): return a + b\"\n",
    "\n",
    "inputs = tokenizer(code, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "print(\"Code embedding shape:\", embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5827275d-4e71-4661-abe1-e047925bbdbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1635,  0.3595,  0.0261,  ..., -0.2831, -0.2772,  0.3455],\n",
       "         [-1.0306,  0.1121,  0.5334,  ..., -0.6773, -0.0715,  0.6466],\n",
       "         [-0.7064,  0.2068, -0.1867,  ..., -0.6525, -0.2080,  0.6185],\n",
       "         ...,\n",
       "         [-0.1911,  0.0171, -0.0912,  ...,  0.1607, -0.5508,  0.4795],\n",
       "         [-0.5272,  0.5221,  0.4231,  ..., -0.7133, -0.6207,  0.9018],\n",
       "         [-0.1643,  0.3608,  0.0268,  ..., -0.2839, -0.2790,  0.3472]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.4756, -0.3428, -0.5196,  0.1175,  0.3530,  0.0713,  0.4972, -0.3266,\n",
       "          0.0949, -0.2564,  0.4359,  0.0017, -0.2382,  0.1062,  0.0036,  0.5640,\n",
       "          0.4242, -0.4990,  0.1064,  0.3437, -0.3256,  0.5128,  0.3463,  0.0351,\n",
       "         -0.0508,  0.2522,  0.1491,  0.1061,  0.5450,  0.1033,  0.1372,  0.0893,\n",
       "          0.1787,  0.0130, -0.3124, -0.0641, -0.5261,  0.1648,  0.6857, -0.2942,\n",
       "         -0.3850,  0.0743,  0.0292, -0.3356,  0.0975,  0.6149,  0.1903,  0.0332,\n",
       "         -0.2491, -0.2047, -0.5238,  0.4482,  0.3531,  0.1766, -0.2673,  0.0625,\n",
       "          0.0425, -0.0769, -0.0922, -0.2918, -0.4435, -0.4421,  0.0875,  0.1742,\n",
       "         -0.1173, -0.1714,  0.4362,  0.0273, -0.3787,  0.1201, -0.3180,  0.2209,\n",
       "          0.0492, -0.7261, -0.1167, -0.0104, -0.6006,  0.1092,  0.6288,  0.4239,\n",
       "         -0.1083,  0.3455, -0.0295,  0.2943, -0.1758, -0.2983,  0.5170, -0.1495,\n",
       "          0.1501,  0.3677, -0.3005, -0.6696, -0.1518,  0.1839, -0.2817,  0.1770,\n",
       "         -0.0071,  0.1231, -0.3540, -0.1028,  0.1535, -0.2123, -0.1553,  0.2810,\n",
       "          0.3292, -0.2595, -0.2251,  0.1073,  0.1776, -0.1517, -0.1506,  0.6412,\n",
       "          0.5424,  0.1266,  0.1552,  0.1129, -0.1996, -0.3696,  0.4036, -0.4031,\n",
       "          0.2316, -0.1797,  0.0543,  0.2946, -0.3526,  0.2192,  0.0964,  0.4775,\n",
       "          0.2190, -0.1140,  0.0370, -0.0271, -0.0265,  0.2212, -0.0270,  0.0515,\n",
       "          0.1717, -0.7449, -0.3842,  0.5739,  0.7721,  0.1305,  0.2401,  0.2364,\n",
       "          0.4831,  0.5701,  0.2405, -0.5751,  0.0508,  0.3219, -0.0393, -0.0805,\n",
       "         -0.2616, -0.4953, -0.6046, -0.0732,  0.3036, -0.0154,  0.0029,  0.5972,\n",
       "          0.2264, -0.4047, -0.2129, -0.1748, -0.2168, -0.3462, -0.1615, -0.0716,\n",
       "         -0.4354, -0.4089,  0.0086, -0.5517, -0.0528,  0.2914, -0.5654,  0.5909,\n",
       "         -0.5212,  0.1680,  0.5068, -0.4094,  0.1320, -0.4729, -0.0332,  0.3873,\n",
       "          0.0717,  0.2697, -0.2941,  0.4612, -0.0263,  0.2119,  0.2330,  0.0360,\n",
       "         -0.3049,  0.3390, -0.4630,  0.1737, -0.3862, -0.1519, -0.3201, -0.3304,\n",
       "          0.2385, -0.8066, -0.5697,  0.0703, -0.1176,  0.0220, -0.0419,  0.0924,\n",
       "          0.0209, -0.1311,  0.0436, -0.4507,  0.3844, -0.2831, -0.2163, -0.0261,\n",
       "          0.3170,  0.4040,  0.2750, -0.5155, -0.3845,  0.2039,  0.4976, -0.1548,\n",
       "          0.5597, -0.1843, -0.2406,  0.0128,  0.3020,  0.1529,  0.5320, -0.3185,\n",
       "         -0.0803,  0.0340, -0.5504, -0.3494, -0.2163,  0.2202,  0.4663,  0.0680,\n",
       "          0.1957,  0.4092,  0.2757, -0.0553,  0.4379, -0.1778,  0.1924, -0.5001,\n",
       "         -0.0328, -0.4469, -0.3525, -0.4836,  0.6934, -0.2943,  0.4941,  0.5081,\n",
       "         -0.3924, -0.2262,  0.2256,  0.1285,  0.1707, -0.0430,  0.0633,  0.1886,\n",
       "          0.0419,  0.3047,  0.5391,  0.2375,  0.3269,  0.0291,  0.0659,  0.3819,\n",
       "         -0.1675, -0.1625, -0.0664,  0.1666, -0.2385, -0.3818,  0.0018, -0.0181,\n",
       "          0.5938, -0.1008, -0.3737,  0.1928,  0.2519, -0.5966,  0.1525,  0.1782,\n",
       "          0.2112, -0.4595, -0.0816,  0.0095,  0.2536, -0.5192, -0.4383,  0.4888,\n",
       "         -0.0195,  0.2657,  0.2749, -0.3558, -0.2573,  0.7496, -0.1178, -0.4819,\n",
       "          0.3547,  0.1344, -0.0014,  0.2237,  0.1922,  0.3624, -0.2635,  0.5110,\n",
       "          0.2180, -0.6102, -0.4524, -0.2351,  0.0049, -0.0026, -0.2923, -0.4978,\n",
       "          0.0782,  0.2172, -0.1936,  0.2848,  0.2390,  0.3270, -0.0175,  0.4520,\n",
       "         -0.2909,  0.5138, -0.0901,  0.5581, -0.5162, -0.0821, -0.0347, -0.1596,\n",
       "         -0.0257, -0.0277,  0.3590, -0.2532, -0.5535,  0.1648,  0.2492,  0.1789,\n",
       "          0.2689,  0.4392,  0.1778,  0.1833,  0.0713,  0.3299,  0.2783, -0.2526,\n",
       "         -0.6879,  0.2829, -0.4324, -0.0933, -0.2451, -0.3212,  0.6961, -0.3357,\n",
       "          0.2370,  0.4157,  0.2355,  0.2179,  0.0727,  0.2713,  0.2225,  0.1710,\n",
       "          0.1108,  0.0060, -0.3838, -0.3588, -0.1746,  0.1541, -0.3669, -0.4942,\n",
       "          0.1621,  0.5392,  0.0841, -0.2827,  0.3847,  0.3428,  0.1864,  0.0200,\n",
       "         -0.2367, -0.0141,  0.6268, -0.0865,  0.2113,  0.7360,  0.2430, -0.4957,\n",
       "         -0.1326, -0.2580,  0.0719,  0.1645, -0.3303,  0.2437,  0.5152,  0.2735,\n",
       "          0.7367,  0.0960,  0.0353,  0.1168,  0.2414, -0.0105,  0.1381,  0.0766,\n",
       "          0.5297,  0.4811, -0.4216,  0.0755, -0.2259, -0.1253, -0.1998, -0.4301,\n",
       "         -0.0359,  0.3544, -0.5722,  0.0695, -0.2190,  0.0905, -0.0605, -0.1019,\n",
       "         -0.2350, -0.3734,  0.6644, -0.0824, -0.0575, -0.4477, -0.3769,  0.0327,\n",
       "          0.2684, -0.3276, -0.2532,  0.4781, -0.0628, -0.0973, -0.1991,  0.2990,\n",
       "         -0.1748,  0.2146,  0.2838, -0.1722, -0.2262,  0.0173, -0.3774, -0.2434,\n",
       "         -0.5185,  0.5410, -0.3504, -0.2017, -0.2923, -0.6155,  0.1348,  0.2762,\n",
       "          0.5086, -0.3067,  0.0555,  0.7995, -0.1169, -0.3508, -0.0176,  0.3845,\n",
       "         -0.0978,  0.6501,  0.5085, -0.0129,  0.1571,  0.6992,  0.0085,  0.2426,\n",
       "         -0.2941,  0.5521, -0.0685,  0.2933,  0.0512,  0.0484,  0.0030, -0.0920,\n",
       "          0.3658,  0.5176, -0.6786, -0.1897,  0.1672,  0.0374, -0.2200, -0.3860,\n",
       "         -0.1989, -0.2985, -0.1231, -0.1292,  0.0125, -0.4859, -0.1161,  0.2722,\n",
       "         -0.4435,  0.1265,  0.5713,  0.0150,  0.3024, -0.4003, -0.1118,  0.3124,\n",
       "         -0.1904, -0.3832,  0.1621,  0.8380, -0.2310, -0.7241, -0.0693,  0.3783,\n",
       "          0.0916,  0.1938, -0.1482, -0.3046,  0.1191,  0.0310,  0.0979, -0.2511,\n",
       "         -0.5477, -0.2714,  0.5865, -0.6033,  0.0773, -0.2068,  0.0047, -0.4351,\n",
       "          0.2615, -0.3511,  0.6718,  0.1931, -0.5437,  0.0861,  0.1015, -0.0563,\n",
       "         -0.1876, -0.2485,  0.7065, -0.4127, -0.7940,  0.2188,  0.2945,  0.5874,\n",
       "         -0.2579, -0.0515, -0.2165,  0.2568,  0.2115,  0.0030, -0.2204, -0.1921,\n",
       "         -0.5586, -0.3308, -0.5324,  0.0677,  0.1084, -0.5759,  0.2247, -0.1994,\n",
       "          0.2929,  0.2456, -0.3833, -0.0812,  0.4267,  0.4559,  0.2367,  0.5163,\n",
       "          0.1963,  0.2112, -0.2343,  0.1588,  0.1913, -0.0713,  0.4482, -0.1089,\n",
       "         -0.6112, -0.0860,  0.7193,  0.0835, -0.2972, -0.0057,  0.5433,  0.1915,\n",
       "          0.0656,  0.2791, -0.3412, -0.2044, -0.0803, -0.0818, -0.4194, -0.2634,\n",
       "         -0.0549, -0.1591, -0.3039, -0.0759, -0.2092,  0.3877, -0.5733, -0.0790,\n",
       "         -0.1612, -0.0428, -0.2753,  0.1054,  0.2024, -0.0127,  0.0915,  0.6384,\n",
       "         -0.1070, -0.1278, -0.1757, -0.2708,  0.2583, -0.3118,  0.1137, -0.1061,\n",
       "          0.3340, -0.5914, -0.2417, -0.0340, -0.2754, -0.4502,  0.4261,  0.2393,\n",
       "          0.1533,  0.5289,  0.1447,  0.0752, -0.3433, -0.3718, -0.2496,  0.2020,\n",
       "          0.0085, -0.4560, -0.2199,  0.2300, -0.4922, -0.2640,  0.3312,  0.0217,\n",
       "         -0.1893, -0.2130, -0.1379, -0.6542,  0.1625,  0.1932,  0.0663, -0.2143,\n",
       "          0.0911, -0.1998,  0.1583,  0.2234,  0.0796, -0.1659, -0.4250, -0.5045,\n",
       "         -0.3007,  0.0838,  0.3544, -0.1643, -0.2337,  0.1361,  0.2893, -0.1783,\n",
       "          0.0750,  0.1341, -0.6376, -0.1653, -0.0337,  0.1143, -0.0548, -0.2661,\n",
       "         -0.4404,  0.2970, -0.1173, -0.2305,  0.5685, -0.2846,  0.3770,  0.0317,\n",
       "         -0.3609, -0.1547,  0.4109, -0.0688,  0.3337,  0.0903, -0.5527, -0.0929,\n",
       "         -0.0441, -0.2234, -0.3407, -0.1449, -0.1684,  0.3344, -0.5954,  0.4285,\n",
       "          0.0804,  0.1920,  0.2183, -0.4253, -0.3177,  0.2711,  0.3775, -0.2775,\n",
       "         -0.5237, -0.4306, -0.3160, -0.3654, -0.2597,  0.5889, -0.1193, -0.2652,\n",
       "         -0.2408,  0.5213,  0.1514,  0.0276,  0.3752,  0.1639,  0.1309,  0.2037,\n",
       "         -0.7100,  0.2317, -0.3598,  0.0121, -0.0517,  0.2165, -0.2654,  0.0800,\n",
       "         -0.2971,  0.1410,  0.4039, -0.3890, -0.1360,  0.3485,  0.2763, -0.3194,\n",
       "          0.0427,  0.2217,  0.3905,  0.1218,  0.1187,  0.4440, -0.4548, -0.0572,\n",
       "         -0.4771, -0.4982,  0.1936,  0.1699,  0.4009, -0.1338, -0.3841,  0.6407,\n",
       "          0.0401, -0.0389,  0.3304, -0.0209, -0.0405, -0.2238,  0.2428,  0.5927,\n",
       "         -0.0700,  0.0495,  0.4528, -0.5662,  0.3881, -0.1075, -0.0193,  0.0437]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:RAG_Demo]",
   "language": "python",
   "name": "conda-env-RAG_Demo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
