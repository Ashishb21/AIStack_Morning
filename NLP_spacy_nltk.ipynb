{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "318c447c-9e4e-4a82-a640-0e912fa2332e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Machine', 'learning'), ('learning', 'is'), ('is', 'fun')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NLTK #####   Bigrams ###\n",
    "\n",
    "#!pip install nltk\n",
    "from nltk import ngrams\n",
    "\n",
    "text = \"Machine learning is fun\".split()\n",
    "list(ngrams(text, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac24cc8d-d585-433b-8b9a-ae480336b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =\"Machine learning is fun\"\n",
    "n=3\n",
    "bigrams = list(ngrams(text.split(), n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb51e2d2-8d8f-4ea3-8837-907069489ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Machine', 'learning', 'is'), ('learning', 'is', 'fun')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ee2977-977b-4d18-a766-5af7048e41d8",
   "metadata": {},
   "source": [
    "## Using Spacy to find bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bce62e2-5b75-4e91-abcc-120ece5e0ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'an'], ['is', 'an', 'example'], ['an', 'example', 'sentence'], ['example', 'sentence', 'for'], ['sentence', 'for', 'creating'], ['for', 'creating', 'n'], ['creating', 'n', '-'], ['n', '-', 'grams'], ['-', 'grams', '.']]\n"
     ]
    }
   ],
   "source": [
    "### Spacy ###\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text =\"This is an example sentence for creating n-grams.\"\n",
    "n=3\n",
    "tokens = [token.text for token in nlp(text)]\n",
    "ngrams = [tokens[i : i + n] for i in range(len(tokens) - n + 1)]\n",
    "print(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc3122f-99e9-48e1-abed-8805d2b59d5c",
   "metadata": {},
   "source": [
    "## Using Scikit Learn for n grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ffc2da6-a14d-4b42-82d3-529aeb0edb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
       "       'second document', 'the first', 'the second', 'the third',\n",
       "       'third one', 'this document', 'this is', 'this the'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## scikit Learn ##\n",
    "#!pip install scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names_out()\n",
    "print(X.toarray())\n",
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "vectorizer2.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5e747-3965-4cd3-8543-08d22148f280",
   "metadata": {},
   "source": [
    "## Tokenize using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6bdfd1e0-8d16-4ee5-8e81-06b5cbf0b854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'NLP', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ashishbansal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "text = \"Tokenization is the first step in NLP!\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46450bf6-3282-4436-b82f-8c5534938a52",
   "metadata": {},
   "source": [
    "## Sentence Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "181ff8bc-3ba6-4d8c-ae2c-f6eb82f8fb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization is important.', 'It splits text into units.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Tokenization is important. It splits text into units.\"\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b17149-6f4b-4c50-80d0-5e1a8764a4ae",
   "metadata": {},
   "source": [
    "## Tokenize using Spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dab22e35-8750-4acf-8d84-7b41d3fe2465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'NLP', '!']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Tokenization is the first step in NLP!\"\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1502646-a76b-4cb7-afb2-4fab962d8367",
   "metadata": {},
   "source": [
    "## sentence token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76669681-6de9-499e-b8a6-fbe71f676cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization is the first step in NLP!']\n"
     ]
    }
   ],
   "source": [
    "sentences = [sent.text for sent in doc.sents]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc0bd29-c807-453c-b455-0c3f98b1888a",
   "metadata": {},
   "source": [
    "\n",
    "## Stop word Removal using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffeec7c5-84bb-44d9-8fff-3ae7096b5146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ashishbansal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/ashishbansal/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e4fb8ca-bd44-43fe-85fe-e6c1afa2facf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simple', 'example', 'demonstrate', 'stopword', 'removal', 'using', 'NLTK', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"This is a simple example to demonstrate stopword removal using NLTK.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc6b8cf-8440-48f2-aaf8-45276ff92500",
   "metadata": {},
   "source": [
    "## Remove Punctuations and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14f72fff-4fdc-4b0b-8a52-06f70ae8e6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simple', 'example', 'demonstrate', 'stopword', 'removal', 'using', 'NLTK']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "filtered_tokens = [\n",
    "    word for word in tokens\n",
    "    if word.lower() not in stop_words and word not in string.punctuation\n",
    "]\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57de402b-d005-45b1-bafd-5e4e1cb9a286",
   "metadata": {},
   "source": [
    "## Stop word Removal using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71072836-a5d0-4c9a-8524-08ba414f7bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66ba51a2-7e13-4a12-82ae-b7821372d2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['simple', 'example', 'demonstrate', 'stop', 'word', 'removal', 'spaCy', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"This is a simple example to demonstrate stop word removal using spaCy.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "filtered_tokens = [\n",
    "    token.text\n",
    "    for token in doc\n",
    "    if not token.is_stop\n",
    "]\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f48904-ac13-471f-8d57-802455310e7e",
   "metadata": {},
   "source": [
    "## stop words removal using scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5227d461-5c01-4baa-ae49-96a9d11858d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['automatically' 'example' 'removal' 'removed' 'shows' 'simple' 'stop'\n",
      " 'word' 'words']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"This is a simple example\",\n",
    "    \"This example shows stop word removal\",\n",
    "    \"Stop words are removed automatically\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98207289-4f40-4043-8e3f-1a21cb1835ef",
   "metadata": {},
   "source": [
    "### Stop words using TFidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24592902-aa91-497b-a7df-593cf9b6daa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['automatically' 'example' 'removal' 'removed' 'shows' 'simple' 'stop'\n",
      " 'word' 'words']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be27835-bee5-4ee6-972c-555ae291a39d",
   "metadata": {},
   "source": [
    "## Stemming using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bc1404b-9e4f-4318-a60f-3cf06e961975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'student', 'were', 'studi', 'and', 'run', 'faster']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ashishbansal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "text = \"The students were studying and running faster\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "stemmed_words = [ps.stem(word) for word in tokens]\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b89904-101e-4aa1-8d2c-f42958b8a8bc",
   "metadata": {},
   "source": [
    "## Lemmatization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "837a2b6b-73fc-4762-a8ad-be66d9728d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ashishbansal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/ashishbansal/nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ashishbansal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ashishbansal/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0978565-abc1-420f-9697-3e37734c366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "running\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cars\"))     # car\n",
    "print(lemmatizer.lemmatize(\"running\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ef80f4-d314-48b6-a707-c1042575e7cd",
   "metadata": {},
   "source": [
    "## Lemitization using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9a35c3-9ad1-423e-91b8-225b68e6c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b3148c5-d03a-4fc7-88f6-4eadcbd4ad83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'student', 'be', 'run', 'fast', 'and', 'eat', 'well', 'food']\n"
     ]
    }
   ],
   "source": [
    "text = \"The students were running faster and eating better food\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "lemmatized = [token.lemma_ for token in doc]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f47f683-484b-44ed-9107-bc2c1ad4c060",
   "metadata": {},
   "source": [
    "## POS tagging Using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2053bde5-8666-412d-910a-66a966ac1cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ashishbansal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/ashishbansal/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e43ffa0-569a-43b6-af9e-c3224154018f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'NNP'), ('is', 'VBZ'), ('looking', 'VBG'), ('at', 'IN'), ('buying', 'VBG'), ('a', 'DT'), ('startup', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "text = \"Apple is looking at buying a startup\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c769ec9-d9b2-482e-86a0-c7825815401e",
   "metadata": {},
   "source": [
    "## Pos Tagging using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ac5d5ef-effa-414d-81a0-365e54ef22dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN NNP\n",
      "is AUX VBZ\n",
      "looking VERB VBG\n",
      "at ADP IN\n",
      "buying VERB VBG\n",
      "a DET DT\n",
      "startup NOUN NN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Apple is looking at buying a startup\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27be6b36-5f33-4098-af2c-77032ba1e3b6",
   "metadata": {},
   "source": [
    "## NER using Nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "18b8d07d-2aa8-48b5-8d87-c0ae08bde4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /Users/ashishbansal/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/ashishbansal/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"maxent_ne_chunker_tab\")\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be8e764a-4e3e-4943-921f-0d0fd591861b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Sundar/NNP)\n",
      "  (PERSON Pichai/NNP)\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  (ORGANIZATION CEO/NN of/IN Google/NNP))\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "text = \"Sundar Pichai is the CEO of Google\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "ner_tree = ne_chunk(pos_tags)\n",
    "\n",
    "print(ner_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c277e48-50a5-47df-bf0d-96fe9b423e6e",
   "metadata": {},
   "source": [
    "## Ner using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9448080-9708-4b7b-8a76-bf52f861cae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sundar Pichai PERSON\n",
      "Google ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Sundar Pichai is the CEO of Google\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b285db5-5cf4-4ca5-b251-9f837263dbac",
   "metadata": {},
   "source": [
    "## Spacy Library for tokenizer , parser NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2106d9b6-664a-463f-9491-d035bb8f686a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
      "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'talk', 'say']\n",
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun GPE\n",
      "Recode ORG\n",
      "earlier this week DATE\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98414cee-d975-4cb2-81ee-38ae0e444aea",
   "metadata": {},
   "source": [
    "## BOW using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cdd5bece-84fc-449c-a3b5-f601c6bc7589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'i': 2, 'love': 2, 'ai': 1, 'and': 1, 'python': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ashishbansal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "text = \"I love AI and I love Python\"\n",
    "\n",
    "tokens = word_tokenize(text.lower())\n",
    "bow = Counter(tokens)\n",
    "\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78199042-a866-4a5b-bab9-bde320ffe4be",
   "metadata": {},
   "source": [
    "## BOW using spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35e813db-4ba9-4a9a-bf30-130467c987a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'love': 2, 'ai': 1, 'python': 1})\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"I love AI and I love Python\"\n",
    "doc = nlp(text.lower())\n",
    "\n",
    "tokens = [\n",
    "    token.lemma_\n",
    "    for token in doc\n",
    "    if not token.is_stop and token.is_alpha\n",
    "]\n",
    "\n",
    "bow = Counter(tokens)\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dca0d9-1a5f-42c0-aaa1-442e48d5f12d",
   "metadata": {},
   "source": [
    "## BOW using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "974dc04d-01e1-4ed4-a522-fe196c2279ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai' 'love' 'loves' 'python']\n",
      "[[1 1 0 0]\n",
      " [0 1 0 1]\n",
      " [1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"I love AI\",\n",
    "    \"I love Python\",\n",
    "    \"AI loves Python\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a52b4e34-c1ae-4e37-9c9a-ec9666fc1d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deep' 'fun' 'is' 'learning' 'machine' 'powerful']\n",
      "[[0.         0.66283998 0.39148397 0.39148397 0.50410689 0.        ]\n",
      " [0.         0.         0.43370786 0.43370786 0.55847784 0.55847784]\n",
      " [0.66283998 0.         0.39148397 0.39148397 0.         0.50410689]]\n"
     ]
    }
   ],
   "source": [
    "## TFidf using sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = [\n",
    "    \"machine learning is fun\",\n",
    "    \"machine learning is powerful\",\n",
    "    \"deep learning is powerful\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70f12657-60bd-4e24-88ff-9afd9c64082a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from gensim) (2.3.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from gensim) (1.16.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/envs/RAG_Demo/lib/python3.11/site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
      "Downloading gensim-4.4.0-cp311-cp311-macosx_11_0_arm64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m  \u001b[33m0:00:10\u001b[0ma \u001b[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gensim\n",
      "Successfully installed gensim-4.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "916dc909-3a8f-4e5c-b0fd-08334071bbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "[('transforming', 0.06797551363706589), ('machine', 0.03364057093858719), ('powerful', 0.009391169995069504), ('ai', 0.0045030321925878525), ('is', -0.010839181020855904), ('technology', -0.023671656847000122), ('deep', -0.11410722881555557), ('fun', -0.11555544286966324)]\n"
     ]
    }
   ],
   "source": [
    "## word2vec\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [\n",
    "    [\"machine\", \"learning\", \"is\", \"fun\"],\n",
    "    [\"deep\", \"learning\", \"is\", \"powerful\"],\n",
    "    [\"ai\", \"is\", \"transforming\", \"technology\"]\n",
    "]\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=1  # 1 = Skip-gram, 0 = CBOW\n",
    ")\n",
    "\n",
    "# Get embedding\n",
    "vector = model.wv[\"learning\"]\n",
    "print(vector.shape)\n",
    "\n",
    "# Similar words\n",
    "print(model.wv.most_similar(\"learning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8440817-3b1f-4a3c-9ef8-4f9fec8fc0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
      "  7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
      " -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
      " -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
      "  6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
      "  2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
      "  6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
      " -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
      " -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
      "  6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
      "  7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
      " -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
      "  1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
      " -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
      "  9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
      " -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
      "  3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
      "  7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
      "  5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
      " -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
      " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
      "  1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
      "  2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
      "  2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
      "  5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n"
     ]
    }
   ],
   "source": [
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41550be1-48c8-42f8-9b6f-f2671440e5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
      "(100,)\n",
      "[('prince', 0.7682329416275024), ('queen', 0.7507690787315369), ('son', 0.7020888328552246), ('brother', 0.6985775828361511), ('monarch', 0.6977890133857727), ('throne', 0.691999077796936), ('kingdom', 0.6811409592628479), ('father', 0.680202841758728), ('emperor', 0.6712858080863953), ('ii', 0.6676074266433716)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "glove = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "vector = glove[\"king\"]\n",
    "print(vector.shape)\n",
    "\n",
    "print(glove.most_similar(\"king\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "922422d7-a2e8-4a71-af61-1872c88a2b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "[ 3.2592006e-04  3.0878314e-04 -2.3986835e-03  1.4634965e-03\n",
      "  1.9383548e-03  2.5352230e-03 -1.5754411e-03  7.8739109e-04\n",
      "  2.8420839e-04  1.7295904e-03 -2.1287976e-03  2.5222203e-04\n",
      " -1.1413345e-03  1.1649579e-03  1.2338966e-03 -2.1058293e-04\n",
      " -4.8548801e-04 -1.0985128e-03  1.0116753e-04 -3.0819492e-03\n",
      " -8.2721905e-04 -1.4776889e-03 -8.4920652e-04  2.1289012e-03\n",
      " -1.8061429e-03 -3.0560524e-03 -2.4155995e-03  5.9602031e-04\n",
      " -2.3576571e-04 -1.5703663e-03 -4.0375376e-03  2.2614432e-04\n",
      " -7.3173677e-04  1.3818332e-03  1.3538908e-04  2.1072933e-03\n",
      "  1.5878961e-03  1.3267922e-03  3.2056745e-05 -7.9966593e-04\n",
      " -5.7083782e-04  1.8891835e-03  1.6194319e-03  6.7433692e-04\n",
      " -1.3552830e-03 -2.1046721e-03 -5.0387275e-04  1.2752799e-03\n",
      " -4.3698688e-05  1.4291970e-03  1.1810130e-03 -7.3555135e-04\n",
      "  3.4552196e-04  1.1330862e-03  1.1542472e-03 -6.2534423e-04\n",
      "  2.7376527e-04 -9.0139161e-04  1.8991953e-03 -1.8430776e-03\n",
      " -1.7570338e-03 -2.5307743e-03  9.1046840e-04 -2.8497418e-03\n",
      " -1.9792165e-03  2.0100025e-04  2.6818758e-03 -2.6438800e-03\n",
      "  7.4435875e-04 -8.1892475e-04 -2.3815938e-04 -8.9270092e-04\n",
      " -1.1180240e-04  6.6764542e-06  9.7750104e-04 -2.3451028e-03\n",
      " -8.8286278e-04 -4.0641127e-04 -9.3453552e-04  3.0236709e-04\n",
      "  7.4439216e-05  4.6795866e-04  1.3906010e-04  1.2250285e-03\n",
      "  2.1600302e-03  3.6620535e-04  2.7081484e-04 -1.9090147e-03\n",
      "  8.6681615e-04 -7.6971017e-04 -1.1321760e-03 -2.6278975e-04\n",
      " -1.9303529e-03 -1.5645348e-03  7.3722488e-04 -1.9692806e-04\n",
      " -6.3594035e-04  8.8380487e-04  2.5793703e-05 -5.0600956e-06]\n"
     ]
    }
   ],
   "source": [
    "## fasttext \n",
    "#Uses character n-grams\n",
    "# Great for noisy text\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "sentences = [\n",
    "    [\"running\", \"is\", \"good\"],\n",
    "    [\"runner\", \"runs\", \"fast\"]\n",
    "]\n",
    "\n",
    "model = FastText(\n",
    "    sentences,\n",
    "    vector_size=100,\n",
    "    window=3,\n",
    "    min_count=1\n",
    ")\n",
    "\n",
    "print(model.wv[\"running\"].shape)\n",
    "\n",
    "# Works even for unseen words\n",
    "print(model.wv[\"runing\"])  # misspelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a676f1-a2be-4df8-8d00-2b55d43e7581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aistack4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
