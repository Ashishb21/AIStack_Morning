{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87fb105f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashishbansal/Documents/Training/Aistack_Morning/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/var/folders/qt/7qfpwbkx3cq0xb9m37811y000000gn/T/ipykernel_15569/810365371.py:5: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-Augmented Generation (RAG) is a machine learning approach that combines the strengths of both retrieval-based and generation-based models. It's an extension of the popular BERT (Bidirectional Encoder Representations from Transformers) architecture, designed to leverage the capabilities of both natural language processing (NLP) and computer vision.\n",
      "\n",
      "In traditional NLP tasks, such as question answering or text classification, models rely on a generator component that generates output based on the input. However, RAG introduces a new component called a \"retrieval head\" or \"matcher,\" which retrieves relevant information from an external knowledge base or database to assist the generation process.\n",
      "\n",
      "The retrieval mechanism uses a neural network to search for and retrieve relevant documents or entities that match the query. This retrieved information is then fed into the generator component, which generates output based on the retrieved information.\n",
      "\n",
      "RAG has several advantages over traditional NLP models:\n",
      "\n",
      "1.  Improved accuracy: By leveraging external knowledge bases, RAG can access a vast amount of context and domain-specific information, leading to more accurate results.\n",
      "2.  Better handling of out-of-vocabulary words: Since RAG retrieves information from a database, it can handle out-of-vocabulary words and entities more effectively than traditional NLP models.\n",
      "3.  Improved interpretability: The retrieval mechanism provides insights into the model's decision-making process, making it easier to understand how the model arrived at its conclusions.\n",
      "\n",
      "RAG has applications in various domains, including:\n",
      "\n",
      "1.  Question answering\n",
      "2.  Text classification\n",
      "3.  Sentiment analysis\n",
      "4.  Entity disambiguation\n",
      "\n",
      "However, RAG also introduces new challenges and limitations, such as:\n",
      "\n",
      "1.  Increased computational requirements: The retrieval mechanism requires additional computations, leading to increased model size and latency.\n",
      "2.  Data requirements: RAG requires access to large knowledge bases or databases, which can be a significant challenge for smaller organizations.\n",
      "\n",
      "Overall, Retrieval Augmented Generation is an exciting area of research that combines the strengths of NLP and computer vision to create more accurate and interpretable models.\n"
     ]
    }
   ],
   "source": [
    "## calling and LLM from Langchain Ollama\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3.2\",\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"What is Retrieval Augmented Generation?\")\n",
    "print(response) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c1c7524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "prompt = [HumanMessage(\"What is the capital of France?\")]\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fdc47ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!Paris!!!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "system_msg = SystemMessage(\n",
    "    \"You are a helpful assistant that responds to questions with three exclamation marks.\"\n",
    ")\n",
    "human_msg = HumanMessage(\"What is the capital of France?\")\n",
    "\n",
    "response = llm.invoke([system_msg, human_msg])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3cd5bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Easy peasy!\n",
      "\n",
      "The answer is: 4\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, SystemMessage, HumanMessage\n",
    "\n",
    "# Create an AI message manually (e.g., for conversation history)\n",
    "ai_msg = AIMessage(\"I'd be happy to help you with that question!\")\n",
    "\n",
    "# Add to conversation history\n",
    "messages = [\n",
    "    SystemMessage(\"You are a helpful assistant\"),\n",
    "    HumanMessage(\"Can you help me?\"),\n",
    "    ai_msg,  # Insert as if it came from the model\n",
    "    HumanMessage(\"Great! What's 2+2?\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a9de245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".\\n\\nContext: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face\\'s `transformers` library, or by utilizing OpenAI and Cohere\\'s offerings through the `openai` and `cohere` libraries, respectively.\\n\\nQuestion: Which model providers offer LLMs?\\n\\nAnswer: '\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "response = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "308ed2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face's `transformers` library, and OpenAI and Cohere.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\")\n",
    "\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "response=llm.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aa5de3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Context: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: Which model providers offer LLMs?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            'Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".',\n",
    "        ),\n",
    "        (\"human\", \"Context: {context}\"),\n",
    "        (\"human\", \"Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffcabbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face's `transformers` library, and OpenAI and Cohere through their respective libraries (`openai` and `cohere`).\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            'Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".',\n",
    "        ),\n",
    "        (\"human\", \"Context: {context}\"),\n",
    "        (\"human\", \"Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "response=llm.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16215c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several model providers that offer Large Language Models (LLMs). Some of the most popular ones include:\n",
      "\n",
      "1. **Hugging Face**: Hugging Face is one of the largest and most well-known model providers for LLMs. They offer a wide range of pre-trained models, including BERT, RoBERTa, and XLNet.\n",
      "2. **Google**: Google offers its own line of LLMs, including BERT, T5, and Bigbird. These models are primarily used in Google's search and language processing applications.\n",
      "3. **Meta AI**: Meta AI is another prominent provider of LLMs. They offer a range of models, including BlenderBot, which is designed for conversational AI.\n",
      "4. **Microsoft**: Microsoft offers its own line of LLMs, including the popular Turing-NLG model.\n",
      "5. **Amazon**: Amazon has also developed several LLMs, including the LexNet and LexRNN models.\n",
      "6. **OpenAI**: OpenAI is perhaps best known for its groundbreaking language model, GPT-3. However, they also offer other LLMs, such as GPT-2 and T5.\n",
      "7. **IBM Watson**: IBM offers a range of LLMs, including the Watson Assistant and Watson Natural Language Understanding models.\n",
      "\n",
      "These are just a few examples of model providers that offer LLMs. There are many others, both large and small, that offer their own models for various applications.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama3.2\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    \n",
    ")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# combine them with the | operator\n",
    "\n",
    "chatbot = template | llm\n",
    "\n",
    "# use it\n",
    "\n",
    "response = chatbot.invoke({\"question\": \"Which model providers offer LLMs?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8c13f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Several\n",
      " model\n",
      " providers\n",
      " offer\n",
      " Large\n",
      " Language\n",
      " Models\n",
      " (\n",
      "LL\n",
      "Ms\n",
      ").\n",
      " Here\n",
      " are\n",
      " some\n",
      " of\n",
      " the\n",
      " most\n",
      " notable\n",
      " ones\n",
      ":\n",
      "\n",
      "\n",
      "1\n",
      ".\n",
      " **\n",
      "Google\n",
      "**:\n",
      " Google\n",
      " has\n",
      " developed\n",
      " several\n",
      " L\n",
      "LM\n",
      "s\n",
      ",\n",
      " including\n",
      " B\n",
      "ERT\n",
      ",\n",
      " Ro\n",
      "BERT\n",
      "a\n",
      ",\n",
      " and\n",
      " T\n",
      "5\n",
      ".\n",
      " These\n",
      " models\n",
      " have\n",
      " been\n",
      " widely\n",
      " adopted\n",
      " in\n",
      " natural\n",
      " language\n",
      " processing\n",
      " tasks\n",
      " such\n",
      " as\n",
      " question\n",
      " answering\n",
      ",\n",
      " sentiment\n",
      " analysis\n",
      ",\n",
      " and\n",
      " text\n",
      " classification\n",
      ".\n",
      "\n",
      "2\n",
      ".\n",
      " **\n",
      "Microsoft\n",
      "**:\n",
      " Microsoft\n",
      " offers\n",
      " a\n",
      " range\n",
      " of\n",
      " L\n",
      "LM\n",
      "s\n",
      " through\n",
      " its\n",
      " Azure\n",
      " Cognitive\n",
      " Services\n",
      " platform\n",
      ",\n",
      " including\n",
      " the\n",
      " Language\n",
      " Understanding\n",
      " Intelligent\n",
      " Service\n",
      " (\n",
      "LU\n",
      "IS\n",
      ")\n",
      " and\n",
      " the\n",
      " Computer\n",
      " Vision\n",
      " API\n",
      ".\n",
      "\n",
      "3\n",
      ".\n",
      " **\n",
      "Amazon\n",
      "**:\n",
      " Amazon\n",
      " provides\n",
      " a\n",
      " range\n",
      " of\n",
      " L\n",
      "LM\n",
      "s\n",
      " through\n",
      " its\n",
      " Amazon\n",
      " Sage\n",
      "Maker\n",
      " service\n",
      ",\n",
      " including\n",
      " the\n",
      " S\n",
      "umer\n",
      "ian\n",
      " deep\n",
      " learning\n",
      " platform\n",
      " and\n",
      " the\n",
      " T\n",
      "extract\n",
      " deep\n",
      " learning\n",
      " platform\n",
      ".\n",
      "\n",
      "4\n",
      ".\n",
      " **\n",
      "H\n",
      "ugging\n",
      " Face\n",
      "**:\n",
      " H\n",
      "ugging\n",
      " Face\n",
      " is\n",
      " an\n",
      " open\n",
      "-source\n",
      " framework\n",
      " that\n",
      " provides\n",
      " pre\n",
      "-trained\n",
      " L\n",
      "LM\n",
      "s\n",
      " for\n",
      " natural\n",
      " language\n",
      " processing\n",
      " tasks\n",
      ",\n",
      " including\n",
      " B\n",
      "ERT\n",
      ",\n",
      " Ro\n",
      "BERT\n",
      "a\n",
      ",\n",
      " and\n",
      " Dist\n",
      "il\n",
      "BERT\n",
      ".\n",
      "\n",
      "5\n",
      ".\n",
      " **\n",
      "Open\n",
      "AI\n",
      "**:\n",
      " Open\n",
      "AI\n",
      " offers\n",
      " a\n",
      " range\n",
      " of\n",
      " L\n",
      "LM\n",
      "s\n",
      " through\n",
      " its\n",
      " D\n",
      "ALL\n",
      "-E\n",
      " and\n",
      " G\n",
      "PT\n",
      "-\n",
      "3\n",
      " models\n",
      ",\n",
      " which\n",
      " have\n",
      " been\n",
      " widely\n",
      " adopted\n",
      " in\n",
      " natural\n",
      " language\n",
      " processing\n",
      " tasks\n",
      " such\n",
      " as\n",
      " text\n",
      " generation\n",
      " and\n",
      " conversation\n",
      " modeling\n",
      ".\n",
      "\n",
      "6\n",
      ".\n",
      " **\n",
      "Meta\n",
      " AI\n",
      "**:\n",
      " Meta\n",
      " AI\n",
      " provides\n",
      " a\n",
      " range\n",
      " of\n",
      " L\n",
      "LM\n",
      "s\n",
      " through\n",
      " its\n",
      " model\n",
      " hub\n",
      ",\n",
      " including\n",
      " the\n",
      " XL\n",
      "Net\n",
      " and\n",
      " Big\n",
      "Bird\n",
      " models\n",
      ",\n",
      " which\n",
      " have\n",
      " been\n",
      " widely\n",
      " adopted\n",
      " in\n",
      " natural\n",
      " language\n",
      " processing\n",
      " tasks\n",
      " such\n",
      " as\n",
      " question\n",
      " answering\n",
      " and\n",
      " sentiment\n",
      " analysis\n",
      ".\n",
      "\n",
      "\n",
      "These\n",
      " are\n",
      " just\n",
      " a\n",
      " few\n",
      " examples\n",
      " of\n",
      " the\n",
      " many\n",
      " model\n",
      " providers\n",
      " that\n",
      " offer\n",
      " L\n",
      "LM\n",
      "s\n",
      ".\n",
      " The\n",
      " landscape\n",
      " is\n",
      " constantly\n",
      " evolving\n",
      ",\n",
      " with\n",
      " new\n",
      " models\n",
      " and\n",
      " providers\n",
      " emerging\n",
      " all\n",
      " the\n",
      " time\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for part in chatbot.stream({\"question\": \"Which model providers offer LLMs?\"}):\n",
    "    print(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ced2e573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As day's final breath descends to sea,\n",
      "The sky aflame, a fiery spree,\n",
      "Orange, pink, and purple hue,\n",
      "A masterpiece, for me and you.\n",
      "\n",
      "The sun sinks low, its light does fade,\n",
      "A burning ember, in the shade,\n",
      "Golden rays upon my face,\n",
      "Warming skin, with a gentle pace.\n",
      "\n",
      "Nature's canvas, painted bright,\n",
      "A kaleidoscope of colors take flight,\n",
      "The world is bathed, in golden glow,\n",
      "As sunset whispers, \"Let go\".\n",
      "\n",
      "The stars appear, one by one,\n",
      "Like diamonds sparkling, beneath the sun,\n",
      "The world is hushed, in quiet sleep,\n",
      "As sunset's final whisper, softly creeps.\n",
      "\n",
      "And as I stand, and watch it fade,\n",
      "I feel the peace, that sunset has made.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Write a short, creative poem about {topic}.\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "result = chain.invoke({\"topic\": \"sunset\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa74c477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'bookName': 'Wings of Fire', 'authorName': 'Tui T. Sutherland', 'publisherName': \"HarperCollins Children's Books\", 'genre': 'Young Adult Fantasy', 'publishingYear': 2012}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"Give me book name, author name, publisher name, genre \n",
    "                and its publishing year of the book\\n\\n{book_name} \\n{format_instructions}\"\"\",\n",
    "\n",
    "    input_variables=[\"book_name\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "result = chain.invoke({\"book_name\": \"Wings of Fire\"})\n",
    "\n",
    "print(type(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24b17353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary=== input_variables=['text'] input_types={} partial_variables={} template='Summarize {text} in 3 points.'\n",
      "Here are three key points summarizing Generative AI:\n",
      "\n",
      "1. **Generative AI creates new content from scratch**: Unlike traditional AI which performs specific tasks, Generative AI uses complex algorithms to learn patterns and generate entirely new content, such as images, text, music, or videos.\n",
      "2. **It's like a super-advanced creative tool**: Generative AI can help produce innovative content that was previously impossible to create by hand, providing users with powerful creative capabilities.\n",
      "3. **It has potential applications in various fields, but also raises questions**: Generative AI has numerous applications in art, design, entertainment, and marketing, but it also raises concerns about creativity, ownership, and the future of work due to its ability to generate new content without human input.\n",
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "        +--------+         \n",
      "        | Ollama |         \n",
      "        +--------+         \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "        +--------+         \n",
      "        | Ollama |         \n",
      "        +--------+         \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "explaination_prompt = PromptTemplate(\n",
    "    template=\"Explain me {topic} in easy language.\",\n",
    "    input_variables=[\"topic\"]\n",
    ")\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "    template=\"Summarize {text} in 3 points.\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = explaination_prompt | llm | parser | summary_prompt | llm | parser\n",
    "print(\"summary===\",summary_prompt)\n",
    "result = chain.invoke({\"topic\": \"Generative AI\"})\n",
    "print(result)\n",
    "\n",
    "chain.get_graph().print_ascii()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Aistack4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
